---
title: Build a Docstring Search Engine
---

In this tutorial you’ll build a semantic search tool on top of `kit`
using **docstring-based indexing**. 

Why docstrings?  Summaries distill *intent* rather than syntax.  Embedding these
short natural-language strings lets the vector DB focus on meaning, giving you
relevant hits even when the literal code differs (e.g., `retry()` vs
`attempt_again()`).  It also keeps the index small (one embedding per file or
symbol instead of dozens of raw-code chunks).

---

## 1. Install dependencies

```bash
uv pip install kit[openai] sentence-transformers chromadb
```

## 2. Initialise a repo and summarizer

```python
import kit
from sentence_transformers import SentenceTransformer

REPO_PATH = "/path/to/your/project"
repo = kit.Repository(REPO_PATH)

summarizer = repo.get_summarizer()  # defaults to OpenAIConfig
```

## 3. Build the docstring index

```python
embed_model = SentenceTransformer("all-MiniLM-L6-v2")
embed_fn = lambda txt: embed_model.encode(txt).tolist()

###############################################################################
#  Choose granularity
#  ------------------
#  • File-level (default) – one summary per file, fastest to build.
#  • Symbol-level        – one summary per function/class; slower but precise.
###############################################################################

indexer = kit.DocstringIndexer(repo, summarizer, embed_fn)
# Build file-level index (fast):
indexer.build()          # writes .kit/docstring_db

# Uncomment for per-symbol summaries:
# indexer.build(level="symbol")
```

The first run will take a few minutes depending on repo size and LLM latency.
Summaries are cached inside the vector DB, so subsequent runs are cheap.

## 4. Query the index

```python
searcher = kit.SummarySearcher(indexer)

results = searcher.search("How is the retry back-off implemented?", top_k=3)
for hit in results:
    print("→", hit["file"], "\n", hit["summary"])
```

You now have a lightweight semantic code searcher – without fine-tuning!

## 5. Next steps

* Use **per-symbol** indexing today – call `indexer.build(level="symbol")`.
* Combine with `repo.search_semantic()` to cross-check raw-code and docstring hits.
* Build a CLI wrapper (`python -m kit.search "your query"`).
* Clear & rebuild the vector DB after big refactors with `indexer.build(force=True)`.
