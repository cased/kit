---
title: AI PR Reviewer
description: Production-ready AI-powered code reviewer with full repository context, transparent pricing, and CI/CD integration
sidebar:
  order: 2
---

import { Aside } from '@astrojs/starlight/components';

# Kit AI PR Reviewer

Kit includes a **production-ready AI PR reviewer** that provides professional-grade code analysis with full repository context. Choose from 15+ models ranging from **$0.00 (free local AI)** to $0.91 per review with complete cost transparency.

<Aside type="tip">
**Want to build a custom reviewer?** See the [Build an AI PR Reviewer tutorial](/tutorials/ai_pr_reviewer) to create your own using kit's components.
</Aside>

## üöÄ Quick Start

```bash
# 1. Install kit (lightweight - no ML dependencies needed for PR review!)
pip install cased-kit

# 2. Set up configuration
kit review --init-config

# 3. Set API keys
export KIT_GITHUB_TOKEN="ghp_your_token"
export KIT_ANTHROPIC_TOKEN="sk-ant-your_key"
export KIT_OPENAI_TOKEN="sk-openai-your_key"
export KIT_GOOGLE_API_KEY="AIzaSy-your_google_key"

# 4. Review any GitHub PR
kit review https://github.com/owner/repo/pull/123

# 5. Test without posting (dry run with full formatting)
kit review --dry-run https://github.com/owner/repo/pull/123

# 6. Get clean output for piping to other tools
kit review --plain https://github.com/owner/repo/pull/123
# Or use the short form:
kit review -p https://github.com/owner/repo/pull/123

# 7. Override model for specific review
kit review --model gpt-4.1-nano https://github.com/owner/repo/pull/123
```

<Aside type="tip">
**Just want the PR reviewer?** The base `pip install cased-kit` gives you everything needed for PR reviews without heavy ML dependencies like PyTorch. If you need semantic search features later, install with `pip install cased-kit[ml]`.
</Aside>

### Real-World Performance
Based on testing against the same complex PR:
- **Speed**: 39-56 seconds (competitive with cloud APIs)
- **Cost**: $0.00 forever (no API usage fees)
- **Privacy**: Code never leaves your infrastructure

<Aside type="note">
**Local AI Trade-offs**: Requires initial setup and 4-8GB disk space per model. But once set up, you get professional-quality reviews with zero ongoing costs and complete privacy.
</Aside>

## üîÑ Output Modes & Piping

Kit provides different output modes for various workflows:

### Standard Mode
```bash
# Posts comment directly to GitHub PR
kit review https://github.com/owner/repo/pull/123
```

### Dry Run Mode (`--dry-run` / `-n`)
```bash
# Shows formatted preview without posting
kit review --dry-run https://github.com/owner/repo/pull/123
```
**Output includes**: Status messages, cost breakdown, quality metrics, and formatted review preview.

### Plain Mode (`--plain` / `-p`) 
```bash
# Clean output perfect for piping to other tools
kit review --plain https://github.com/owner/repo/pull/123
kit review -p https://github.com/owner/repo/pull/123
```
**Output**: Just the raw review content with no status messages or formatting.

### Piping to CLI Code Writers

Combine kit's repository intelligence with your favorite CLI code-generation tooling:

```bash
# Analyze with kit, implement with Claude Code
kit review -p https://github.com/owner/repo/pull/123 | \
  claude -p "Implement all the suggestions from this code review"

# Use specific models for cost optimization
kit review -p --model gpt-4.1-nano https://github.com/owner/repo/pull/123 | \
  claude -p "Fix the high-priority issues mentioned in this review"
```

### Integration Examples

**Pipe to any tool:**
```bash
# Save review to file
kit review -p <pr-url> > review.md

# Send to Slack
kit review -p <pr-url> | curl -X POST -H 'Content-type: application/json' \
  --data '{"text":"'"$(cat)"'"}' YOUR_SLACK_WEBHOOK

# Process with jq or other tools
kit review -p <pr-url> | your-custom-processor
```

**Multi-stage workflows:**
```bash
# Stage 1: Kit analyzes codebase context  
REVIEW=$(kit review -p --model claude-3-5-haiku-20241022 <pr-url>)

# Stage 2: Claude Code implements fixes
echo "$REVIEW" | claude -p "Implement these code review suggestions"

# Stage 3: Your custom tooling
echo "$REVIEW" | your-priority-tracker --extract-issues
```

<Aside type="tip">
**Why this workflow rocks**: Kit excels at codebase understanding and context, while Claude Code excels at implementation. Combining them gives you AI-powered analysis ‚Üí AI-powered fixes in seconds.
</Aside>

## üí∞ Transparent Pricing

Based on real-world testing on production open source PRs:

### Model Options (Large PR Example)

| Model | Typical Cost | Quality | Best For |
|-------|------|---------|----------|
| **qwen2.5-coder:latest** | **$0.00** | ‚≠ê‚≠ê‚≠ê‚≠ê | **Free local AI |
| **gemini-1.5-flash-8b** | **$0.003** | ‚≠ê‚≠ê‚≠ê | Ultra-budget, high volume |
| **gpt-4.1-nano** | **$0.0015-0.004** | ‚≠ê‚≠ê‚≠ê | High-volume, ultra-budget |
| **gpt-4.1-mini** | **$0.005-0.015** | ‚≠ê‚≠ê‚≠ê‚≠ê | Budget-friendly, often very good for the price |
| **gemini-2.5-flash** | **$0.007** | ‚≠ê‚≠ê‚≠ê‚≠ê | Excellent value, fast |
| **gpt-4.1** | **$0.02-0.10** | ‚≠ê‚≠ê‚≠ê‚≠ê | Good value |
| **claude-sonnet-4** | **0.08-$0.14** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Recommended for most** |

### In practice

Even without optimizing your model mix (see below), a team doing 500 
large PRs a month will generally pay under $50 a month total (yes)
for reviews with SOTA models; and likely less. 

<Aside type="tip">
Don't underestimate the smaller models.
gpt-4.1-mini (or even nano!) delivers surprisingly useful reviews *when given the right context via kit*. For simple projects, you can get decent AI code reviews for **less than $1/month**.
Here's an example [against kit itself](https://github.com/cased/kit/pull/56#issuecomment-2928399599). This review cost *half a cent*.
</Aside>

### Free Local AI with Ollama

**Zero-cost PR reviews** using state-of-the-art local models. 
Perfect for teams who want professional code analysis at zero cost and without sending code to external APIs.

```bash
# 1. Install Ollama (one-time setup)
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Pull a model, take your pick!
ollama pull qwen2.5-coder:latest

# 3. Configure kit for Ollama
export KIT_GITHUB_TOKEN="ghp_your_token"  # Still need GitHub API access

# 4. Review with free local AI
kit review --model qwen2.5-coder:latest https://github.com/owner/repo/pull/123
```

## üéØ Key Features

### Intelligent Analysis
- **Repository Context**: Full codebase understanding, not just diff analysis
- **Symbol Analysis**: Identifies when functions/classes are used elsewhere  
- **Cross-Impact Assessment**: Understands how changes affect the broader system
- **Multi-Language Support**: Works with any language kit supports

### Professional Output
- **Priority-Based Issues**: High/Medium/Low issue categorization
- **Specific Recommendations**: Concrete code suggestions with examples
- **GitHub Integration**: Clickable links to all referenced files
- **Quality Scoring**: Objective metrics for review effectiveness

### Cost & Transparency
- **Real-Time Cost Tracking**: See exact LLM usage and costs
- **Token Breakdown**: Understand what drives costs
- **Model Information**: Know which AI provided the analysis
- **No Hidden Fees**: Pay only for actual LLM usage

## üîß Configuration

### Model Override via CLI

Override the model for any specific review without modifying your configuration:

```bash
kit review --model gpt-4.1-nano https://github.com/owner/repo/pull/123
kit review --model gpt-4.1 https://github.com/owner/repo/pull/123

# Short flag also works
kit review -m claude-sonnet-4-20250514 https://github.com/owner/repo/pull/123
```

**Available Models:**
- **Ollama (Free Local)**: `qwen2.5-coder:latest`, `deepseek-r1:latest`, `gemma3:latest`, `devstral:latest`, etc
- **OpenAI**: `gpt-4.1-nano`, `gpt-4.1-mini`, `gpt-4.1`, `gpt-4o-mini`, `gpt-4o`
- **Anthropic**: `claude-3-5-haiku-20241022`, `claude-3-5-sonnet-20241022`, `claude-opus-4-20250514`, `claude-sonnet-4-20250514`
- **Google**: `gemini-1.5-flash-8b`, `gemini-2.5-flash`, `gemini-1.5-flash`, `gemini-1.5-pro`, `gemini-2.5-pro`

<Aside type="tip">
**Model validation**: Kit automatically validates model names and provides helpful suggestions if you mistype. Try `kit review --model gpt4` to see the validation in action!
</Aside>

<Aside type="tip">
**Pro tip**: Use different models based on PR complexity. Save `claude-opus-4` for architectural changes and use `gpt-4.1-nano` for documentation/minor fixes.
</Aside>

### Setup API Keys

**GitHub Token**: Get from [GitHub Settings ‚Üí Developer settings ‚Üí Personal access tokens](https://github.com/settings/tokens)
```bash
export KIT_GITHUB_TOKEN="ghp_your_token_here"
```

**LLM API Keys**: 
```bash
# For Anthropic Claude (recommended)
export KIT_ANTHROPIC_TOKEN="sk-ant-your_key"

# For OpenAI GPT models
export KIT_OPENAI_TOKEN="sk-your_openai_key"

# For Google Gemini models
export KIT_GOOGLE_API_KEY="AIzaSy-your_google_key"
```

### Configuration File

Edit `~/.kit/review-config.yaml`:

**Cloud API Configuration:**
```yaml
github:
  token: ghp_your_token_here
  base_url: https://api.github.com

llm:
  provider: anthropic  # or "openai", "google"
  model: claude-sonnet-4-20250514
  api_key: sk-ant-your_key_here
  max_tokens: 4000
  temperature: 0.1

review:
  post_as_comment: true
  clone_for_analysis: true
  cache_repos: true
  max_files: 50
```

**Google Gemini Configuration:**
```yaml
github:
  token: ghp_your_token_here
  base_url: https://api.github.com

llm:
  provider: google
  model: gemini-2.5-flash  # or gemini-1.5-flash-8b for ultra-budget
  api_key: AIzaSy-your_google_key
  max_tokens: 4000
  temperature: 0.1

review:
  post_as_comment: true
  clone_for_analysis: true
  cache_repos: true
  max_files: 50
```

**Free Local AI Configuration (Ollama):**
```yaml
github:
  token: ghp_your_token_here  # Still need GitHub API access
  base_url: https://api.github.com

llm:
  provider: ollama
  model: qwen2.5-coder:latest  # or deepseek-r1:latest
  api_base_url: http://localhost:11434
  api_key: ollama  # Placeholder (Ollama doesn't use API keys)
  max_tokens: 4000
  temperature: 0.1

review:
  post_as_comment: true
  clone_for_analysis: true
  cache_repos: true
  max_files: 50
```

## üìä Review Examples

See real-world examples with actual costs and analysis:

- **[FastAPI Packaging Change](https://github.com/cased/kit/blob/main/src/kit/pr_review/example_reviews/fastapi_11935_standard_dependencies.md)** ($0.034) - Architectural impact analysis
- **[React.dev UI Feature](https://github.com/cased/kit/blob/main/src/kit/pr_review/example_reviews/react_dev_6986_branding_menu.md)** ($0.012) - Accessibility-focused review  
- **[Documentation Fix](https://github.com/cased/kit/blob/main/src/kit/pr_review/example_reviews/biopython_204_documentation_fix.md)** ($0.006) - Proportional response
- **[Multi-Model Comparison](https://github.com/cased/kit/blob/main/src/kit/pr_review/example_reviews/model_comparison_fastapi_11935.md)** - Cost vs quality analysis

## üöÄ CI/CD Integration

### GitHub Actions

Create `.github/workflows/pr-review.yml`:

```yaml
name: AI PR Review
on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  ai-review:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: read
    
    steps:
      - name: AI Code Review
        run: |
          pip install cased-kit
          kit review ${{ github.event.pull_request.html_url }}
        env:
          KIT_GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          KIT_ANTHROPIC_TOKEN: ${{ secrets.ANTHROPIC_API_KEY }}
```

### Advanced Workflows

**Free Local AI Setup** (Self-hosted runners with Ollama):
```yaml
- name: Free AI Review with Ollama
  runs-on: self-hosted  # Requires self-hosted runner with Ollama installed
  steps:
    - name: AI Code Review
      run: |
        pip install cased-kit
        # Use completely free local AI
        kit review --model qwen2.5-coder:latest ${{ github.event.pull_request.html_url }}
      env:
        KIT_GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        # No LLM API keys needed - Ollama is free!
```

**Budget-Conscious Setup** (GPT-4.1-nano):
```yaml
- name: Budget AI Review
  run: |
    pip install cased-kit
    # Configure for ultra-low cost
    kit review --model gpt-4.1-nano ${{ github.event.pull_request.html_url }}
```

**Model-Based on PR Size**:
```yaml
- name: Smart Model Selection
  run: |
    pip install cased-kit
    # Use budget model for small PRs, premium for large ones
    FILES_CHANGED=$(gh pr view ${{ github.event.pull_request.number }} --json files --jq '.files | length')
    if [ "$FILES_CHANGED" -gt 10 ]; then
      MODEL="claude-sonnet-4-20250514"
    else
      MODEL="gpt-4o-mini"
    fi
    kit review --model "$MODEL" ${{ github.event.pull_request.html_url }}
  env:
    GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

**Multi-Stage Analysis with Piping**:
```yaml
- name: AI Review with Implementation
  run: |
    pip install cased-kit
    # Stage 1: Generate review with kit's repository intelligence
    REVIEW=$(kit review -p --model claude-3-5-haiku-20241022 ${{ github.event.pull_request.html_url }})
    
    # Stage 2: Extract action items and post as separate comment
    echo "$REVIEW" | your-issue-tracker --extract-priorities | \
      gh pr comment ${{ github.event.pull_request.number }} --body-file -
    
    # Stage 3: Save review for later processing
    echo "$REVIEW" > review-${{ github.event.pull_request.number }}.md
    
    # Stage 4: Send to team notification system
    echo "$REVIEW" | your-slack-notifier --channel engineering
  env:
    GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    KIT_ANTHROPIC_TOKEN: ${{ secrets.ANTHROPIC_API_KEY }}
```

**Integration with External Tools**:
```yaml
- name: Review and Process
  run: |
    pip install cased-kit
    # Get clean review output for processing
    kit review -p ${{ github.event.pull_request.html_url }} > raw-review.txt
    
    # Parse with your custom tools
    python scripts/extract-security-issues.py raw-review.txt
    python scripts/update-team-dashboard.py raw-review.txt
    
    # Optional: Post processed results back to PR
    gh pr comment ${{ github.event.pull_request.number }} --body-file processed-summary.md
```

**Conditional Reviews**:
```yaml
# Only review non-draft PRs
- name: AI Review
  if: "!github.event.pull_request.draft"
  
# Only review PRs with specific labels  
- name: AI Review
  if: contains(github.event.pull_request.labels.*.name, 'needs-review')
  
# Use premium model for breaking changes
- name: Premium Review for Breaking Changes
  if: contains(github.event.pull_request.labels.*.name, 'breaking-change')
  run: |
    pip install cased-kit
    kit review --model claude-opus-4-20250514 ${{ github.event.pull_request.html_url }}
  env:
    KIT_GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    KIT_ANTHROPIC_TOKEN: ${{ secrets.ANTHROPIC_API_KEY }}
```

## üìà What's Next: Roadmap

#### Custom Context & Learning
- **Per-Organization Context**: Store custom guidelines and coding standards
- **Feedback Learning**: Simple database to learn from review feedback
- **Inline Comments**: Post comments directly on specific lines
- **Follow-up review awareness**: Take previous reviews into account for better feedback

```bash
# Coming soon
kit profile create --name "company-standards" --file coding-guidelines.md
kit review --profile company-standards <pr-url>
kit feedback <review-id> --helpful --notes "Great catch!"
```

```bash
# Future features
kit review <pr-url> --consensus  # Multiple models
kit review <pr-url> --mode inline  # Line-level comments
```

## üîç Advanced Usage

### Cache Management
```bash
# Check cache status
kit review-cache status

# Clean up old repositories  
kit review-cache cleanup

# Clear all cached repositories
kit review-cache clear
```

### Quality Validation

Every review includes objective quality scoring:
- **File References**: Checks if review references actual changed files
- **Specificity**: Measures concrete vs vague feedback  
- **Coverage**: Assesses if major changes are addressed
- **Relevance**: Ensures suggestions align with actual code changes

## üí° Best Practices

### Cost Optimization
- **Use free local AI** for unlimited reviews with Ollama (requires self-hosted setup)
- Use budget models for routine changes, premium for breaking changes
- Use the `--model` flag to override models per PR: `kit review --model gpt-4.1-nano <pr-url>`
- Leverage caching - repeat reviews of same repo are 5-10x faster
- Set up profiles to avoid redundant context

### Output Mode Selection
- **Standard mode**: For direct GitHub integration and team collaboration
- **Dry run (`--dry-run`)**: For testing configurations and seeing full diagnostics
- **Plain mode (`--plain` / `-p`)**: For piping to CLI code composers, custom tools, or automation
- **Tip**: Use plain mode in CI/CD for processing reviews with external systems

### Piping & Integration Workflows
- **Kit ‚Üí Claude Code**: `kit review -p <pr-url> | claude "implement these suggestions"`
- **Multi-tool chains**: Use kit for analysis, pipe to specialized tools for processing
- **Cost optimization**: Use budget models for analysis, premium models for implementation
- **Automation**: Pipe plain output to issue trackers, notification systems, or custom processors

### Team Adoption
- **Start with free local AI** to build confidence without costs
- Start with dry runs to build confidence
- Use budget models initially to control costs
- Create organization-specific guidelines for consistent reviews

### Integration
- Add to CI/CD for all PRs or just high-impact branches
- Use conditional logic to avoid reviewing bot PRs or documentation-only changes
- Monitor costs and adjust model selection based on team needs
- **Consider self-hosted runners** with Ollama for zero ongoing LLM costs

---

The kit AI PR reviewer provides **professional-grade code analysis** at costs accessible to any team size, from **$0.00/month with free local AI** to enterprise-scale deployment. With full repository context and transparent pricing, it's designed to enhance your development workflow without breaking the budget. 