---
title: AI PR Reviewer
description: Production-ready AI-powered code reviewer with full repository context, transparent pricing, and CI/CD integration
sidebar:
  order: 2
---

import { Aside } from '@astrojs/starlight/components';

# Kit AI PR Reviewer

Kit includes a **production-ready AI PR reviewer** that provides professional-grade code analysis with full repository context. Choose from 10 models ranging from $0.005 to $0.91 per review with complete cost transparency.

<Aside type="tip">
**Want to build a custom reviewer?** See the [Build an AI PR Reviewer tutorial](/tutorials/ai_pr_reviewer) to create your own using kit's components.
</Aside>

## üöÄ Quick Start

```bash
# 1. Install kit (lightweight - no ML dependencies needed for PR review!)
pip install cased-kit

# 2. Set up configuration
kit review --init-config

# 3. Set API keys
export KIT_GITHUB_TOKEN="ghp_your_token"
export KIT_ANTHROPIC_TOKEN="sk-ant-your_key"

# 4. Review any GitHub PR
kit review https://github.com/owner/repo/pull/123

# 5. Test without posting (dry run)
kit review --dry-run https://github.com/owner/repo/pull/123

# 6. Override model for specific review
kit review --model gpt-4.1-nano https://github.com/owner/repo/pull/123
```

<Aside type="tip">
**Just want the PR reviewer?** The base `pip install cased-kit` gives you everything needed for PR reviews without heavy ML dependencies like PyTorch. If you need semantic search features later, install with `pip install cased-kit[ml]`.
</Aside>

## üí∞ Transparent Pricing

Based on real-world testing on production open source PRs:

### Model Options (Large PR Example)

| Model | Typical Cost | Quality | Best For |
|-------|------|---------|----------|
| **gpt-4.1-nano** | **$0.0015-0.004** | ‚≠ê‚≠ê‚≠ê | High-volume, ultra-budget |
| **gpt-4.1-mini** | **$0.005-0.015** | ‚≠ê‚≠ê‚≠ê‚≠ê | Budget-friendly, often very good for the price |
| **gpt-4.1** | **$0.02-0.10** | ‚≠ê‚≠ê‚≠ê‚≠ê | Good value |
| **claude-sonnet-4** | **0.08-$0.14** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Recommended for most** |

### In practice

Even without optimizing your model mix (see below), a team doing 500 
large PRs a month will generally pay under $50 a month total (yes)
for reviews with SOTA models; and likely less. 

<Aside type="tip">
Don't underestimate the smaller models.
gpt-4.1-mini (or even nano!) delivers surprisingly useful reviews *when given the right context via kit*. For simple projects, you can get decent AI code reviews for **less than $1/month**.
Here's an example [against kit itself](https://github.com/cased/kit/pull/56#issuecomment-2928399599). This review cost *half a cent*.
</Aside>

## üéØ Key Features

### Intelligent Analysis
- **Repository Context**: Full codebase understanding, not just diff analysis
- **Symbol Analysis**: Identifies when functions/classes are used elsewhere  
- **Cross-Impact Assessment**: Understands how changes affect the broader system
- **Multi-Language Support**: Works with any language kit supports

### Professional Output
- **Priority-Based Issues**: High/Medium/Low issue categorization
- **Specific Recommendations**: Concrete code suggestions with examples
- **GitHub Integration**: Clickable links to all referenced files
- **Quality Scoring**: Objective metrics for review effectiveness

### Cost & Transparency
- **Real-Time Cost Tracking**: See exact LLM usage and costs
- **Token Breakdown**: Understand what drives costs
- **Model Information**: Know which AI provided the analysis
- **No Hidden Fees**: Pay only for actual LLM usage

## üîß Configuration

### Model Override via CLI

Override the model for any specific review without modifying your configuration:

```bash
kit review --model gpt-4.1-nano https://github.com/owner/repo/pull/123
kit review --model gpt-4.1 https://github.com/owner/repo/pull/123

# Short flag also works
kit review -m claude-sonnet-4-20250514 https://github.com/owner/repo/pull/123
```

**Available Models:**
- OpenAI: `gpt-4.1-nano`, `gpt-4.1-mini`, `gpt-4.1`, `gpt-4o-mini`, `gpt-4o`
- Anthropic: `claude-3-5-haiku-20241022`, `claude-3-5-sonnet-20241022`, `claude-opus-4-20250514`, `claude-sonnet-4-20250514`

<Aside type="tip">
**Model validation**: Kit automatically validates model names and provides helpful suggestions if you mistype. Try `kit review --model gpt4` to see the validation in action!
</Aside>

<Aside type="tip">
**Pro tip**: Use different models based on PR complexity. Save `claude-opus-4` for architectural changes and use `gpt-4.1-nano` for documentation/minor fixes.
</Aside>

### Setup API Keys

**GitHub Token**: Get from [GitHub Settings ‚Üí Developer settings ‚Üí Personal access tokens](https://github.com/settings/tokens)
```bash
export KIT_GITHUB_TOKEN="ghp_your_token_here"
```

**LLM API Keys**: 
```bash
# For Anthropic Claude (recommended)
export KIT_ANTHROPIC_TOKEN="sk-ant-your_key"

# For OpenAI GPT models
export KIT_OPENAI_TOKEN="sk-your_openai_key"
```

### Configuration File

Edit `~/.kit/review-config.yaml`:

```yaml
github:
  token: ghp_your_token_here
  base_url: https://api.github.com

llm:
  provider: anthropic  # or "openai"
  model: claude-sonnet-4-20250514
  api_key: sk-ant-your_key_here
  max_tokens: 4000
  temperature: 0.1

review:
  post_as_comment: true
  clone_for_analysis: true
  cache_repos: true
  max_files: 50
```

## üìä Review Examples

See real-world examples with actual costs and analysis:

- **[FastAPI Packaging Change](https://github.com/cased/kit/blob/main/src/kit/pr_review/example_reviews/fastapi_11935_standard_dependencies.md)** ($0.034) - Architectural impact analysis
- **[React.dev UI Feature](https://github.com/cased/kit/blob/main/src/kit/pr_review/example_reviews/react_dev_6986_branding_menu.md)** ($0.012) - Accessibility-focused review  
- **[Documentation Fix](https://github.com/cased/kit/blob/main/src/kit/pr_review/example_reviews/biopython_204_documentation_fix.md)** ($0.006) - Proportional response
- **[Multi-Model Comparison](https://github.com/cased/kit/blob/main/src/kit/pr_review/example_reviews/model_comparison_fastapi_11935.md)** - Cost vs quality analysis

## üöÄ CI/CD Integration

### GitHub Actions

Create `.github/workflows/pr-review.yml`:

```yaml
name: AI PR Review
on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  ai-review:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: read
    
    steps:
      - name: AI Code Review
        run: |
          pip install cased-kit
          kit review ${{ github.event.pull_request.html_url }}
        env:
          KIT_GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          KIT_ANTHROPIC_TOKEN: ${{ secrets.ANTHROPIC_API_KEY }}
```

### Advanced Workflows

**Budget-Conscious Setup** (GPT-4.1-nano):
```yaml
- name: Budget AI Review
  run: |
    pip install cased-kit
    # Configure for ultra-low cost
    kit review --model gpt-4.1-nano ${{ github.event.pull_request.html_url }}
```

**Model-Based on PR Size**:
```yaml
- name: Smart Model Selection
  run: |
    pip install cased-kit
    # Use budget model for small PRs, premium for large ones
    FILES_CHANGED=$(gh pr view ${{ github.event.pull_request.number }} --json files --jq '.files | length')
    if [ "$FILES_CHANGED" -gt 10 ]; then
      MODEL="claude-sonnet-4-20250514"
    else
      MODEL="gpt-4o-mini"
    fi
    kit review --model "$MODEL" ${{ github.event.pull_request.html_url }}
  env:
    GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

**Conditional Reviews**:
```yaml
# Only review non-draft PRs
- name: AI Review
  if: "!github.event.pull_request.draft"
  
# Only review PRs with specific labels  
- name: AI Review
  if: contains(github.event.pull_request.labels.*.name, 'needs-review')
  
# Use premium model for breaking changes
- name: Premium Review for Breaking Changes
  if: contains(github.event.pull_request.labels.*.name, 'breaking-change')
  run: |
    pip install cased-kit
    kit review --model claude-opus-4-20250514 ${{ github.event.pull_request.html_url }}
  env:
    KIT_GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    KIT_ANTHROPIC_TOKEN: ${{ secrets.ANTHROPIC_API_KEY }}
```

## üìà What's Next: Roadmap

#### Custom Context & Learning
- **Per-Organization Context**: Store custom guidelines and coding standards
- **Feedback Learning**: Simple database to learn from review feedback
- **Inline Comments**: Post comments directly on specific lines
- **Follow-up review awareness**: Take previous reviews into account for better feedback

```bash
# Coming soon
kit profile create --name "company-standards" --file coding-guidelines.md
kit review --profile company-standards <pr-url>
kit feedback <review-id> --helpful --notes "Great catch!"
```

```bash
# Future features
kit review <pr-url> --consensus  # Multiple models
kit review <pr-url> --mode inline  # Line-level comments
```

## üîç Advanced Usage

### Cache Management
```bash
# Check cache status
kit review-cache status

# Clean up old repositories  
kit review-cache cleanup

# Clear all cached repositories
kit review-cache clear
```

### Quality Validation

Every review includes objective quality scoring:
- **File References**: Checks if review references actual changed files
- **Specificity**: Measures concrete vs vague feedback  
- **Coverage**: Assesses if major changes are addressed
- **Relevance**: Ensures suggestions align with actual code changes

## üí° Best Practices

### Cost Optimization
- Use budget models for routine changes, premium for breaking changes
- Use the `--model` flag to override models per PR: `kit review --model gpt-4.1-nano <pr-url>`
- Leverage caching - repeat reviews of same repo are 5-10x faster
- Set up profiles to avoid redundant context

### Team Adoption
- Start with dry runs to build confidence
- Use budget models initially to control costs
- Create organization-specific guidelines for consistent reviews

### Integration
- Add to CI/CD for all PRs or just high-impact branches
- Use conditional logic to avoid reviewing bot PRs or documentation-only changes
- Monitor costs and adjust model selection based on team needs

---

The kit AI PR reviewer provides **professional-grade code analysis** at costs accessible to any team size, from $0.46/month for small teams to enterprise-scale deployment. With full repository context and transparent pricing, it's designed to enhance your development workflow without breaking the budget. 