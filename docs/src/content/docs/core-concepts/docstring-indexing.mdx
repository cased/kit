---
title: Docstring-based Vector Indexing
---
import { Steps, Aside } from '@astrojs/starlight/components';

<Aside type="note" title="Alpha Feature">
  The features described on this page, particularly symbol-level indexing and LLM-generated summaries, are currently in **alpha**. API and behavior may change in future releases. Please use with this in mind and report any issues or feedback.
</Aside>

`DocstringIndexer` builds a vector index using **LLM-generated summaries** of
source files ("docstrings") instead of the raw code.  This often yields more
relevant results because the embedded text focuses on *intent* rather than
syntax or specific variable names.

## Why use it?

* **Cleaner embeddings** – Comments like *“Summary of retry logic”* embed better
  than nested `for`-loops.
* **Smaller index** – One summary per file (or symbol) is < 1 kB, while the file
  itself might be thousands of tokens.
* **Provider-agnostic** – Works with any LLM supported by `kit.Summarizer`
  (OpenAI, Anthropic, Google…).

## How it Works

1.  **Configuration**: Instantiate `DocstringIndexer` with a `Repository` object,
a   `Summarizer` (configured with your desired LLM, e.g., OpenAI, Anthropic, Google),
    and an embedding function (`embed_fn`).

    ```python
    from kit import Repository, DocstringIndexer, Summarizer
    from kit.embeddings import VoyageAIEmbeddings # Or your preferred embedder
    from kit.llms.openai import OpenAIConfig

    repo = Repository("/path/to/your/code")
    llm_config = OpenAIConfig(model="gpt-3.5-turbo")
    summarizer = Summarizer(repo, config=llm_config)
    embed_fn = VoyageAIEmbeddings().embed_document # Example embedding function

    indexer = DocstringIndexer(repo, summarizer, embed_fn)
    ```

2.  **Building the Index**: Call `indexer.build()`.
    By default, this creates summaries for each *file*.

    ```python
    # Build a file-level index (default)
    indexer.build()
    ```

    You can also specify `file_extensions` to only index certain file types:
    ```python
    indexer.build(file_extensions=[".py", ".md"])
    ```

### Symbol-Level Indexing

<Aside type="caution" title="Alpha Feature: Symbol-Level Indexing">
  Symbol-level indexing is an advanced alpha feature. While powerful, it may require more resources and is undergoing active development. Feedback is highly appreciated.
</Aside>

For more granular search, you can instruct `DocstringIndexer` to create summaries for individual **functions and classes** within your files. This allows for highly specific semantic queries like "find the class that manages database connections" or "what function handles user authentication?"

To enable symbol-level indexing, pass `level="symbol"` to `build()`:

```python
# Build a symbol-level index
indexer.build(level="symbol", file_extensions=[".py"], force=True)
```

When `level="symbol"`:
*   `DocstringIndexer` iterates through files, then extracts symbols (functions, classes) from each file using `repo.extract_symbols()`.
*   It then calls `summarizer.summarize_function()` or `summarizer.summarize_class()` for each symbol.
*   The resulting embeddings are stored with metadata including:
    *   `file_path`: The path to the file containing the symbol.
    *   `symbol_name`: The name of the function or class (e.g., `my_function`, `MyClass`, `MyClass.my_method`).
    *   `symbol_type`: The type of symbol (e.g., "FUNCTION", "CLASS", "METHOD").
    *   `summary`: The LLM-generated summary of the symbol.
    *   `level`: Set to `"symbol"`.

3.  **Querying**: Use `SummarySearcher` to find relevant summaries.

    ```python
    from kit import SummarySearcher

    searcher = SummarySearcher(indexer) # Pass the built indexer
    results = searcher.search("user authentication logic", top_k=3)

    for res in results:
        print(f"Score: {res['score']:.4f}")
        if res.get('level') == 'symbol':
            print(f"  Symbol: {res['symbol_name']} ({res['symbol_type']}) in {res['file_path']}")
        else:
            print(f"  File: {res['file_path']}")
        print(f"  Summary: {res['summary'][:100]}...")
        print("---")
    ```
    The `results` will contain the summary and associated metadata, including the `level` and symbol details if applicable.

## Quick start

```python
import kit
from sentence_transformers import SentenceTransformer

repo = kit.Repository("/path/to/your/project")

# 1. LLM summarizer (make sure OPENAI_API_KEY / etc. is set)
summarizer = repo.get_summarizer()

# 2. Embedding function (any model that returns list[float])
embed_model = SentenceTransformer("all-MiniLM-L6-v2")
embed_fn = lambda txt: embed_model.encode(txt).tolist()

# 3. Build the index (stored in .kit/docstring_db)
indexer = kit.DocstringIndexer(repo, summarizer, embed_fn)
indexer.build()

# 4. Search
searcher = kit.SummarySearcher(indexer)
for hit in searcher.search("retry back-off", top_k=5):
    print(hit["file"], "→", hit["summary"])
```

### Storage details

`DocstringIndexer` delegates persistence to any
`kit.vector_searcher.VectorDBBackend`.  The default backend is
[`Chroma`](https://docs.trychroma.com/) and lives in
`.kit/docstring_db/` inside your repo.

## Use Cases

*   **Semantic Code Search**: Find code by describing what it *does*, not just
    what keywords it contains. (e.g., “retry back-off logic” instead of trying to
    guess variable names like `exponential_delay` or `MAX_RETRIES`).
*   **Onboarding**: Quickly understand what different parts of a codebase are for.
*   **Automated Documentation**: Use the summaries as a starting point for API docs.
*   **Codebase Q&A**: As shown in the [Codebase Q&A Bot tutorial](/docs/tutorials/codebase-qa-bot), combine `SummarySearcher` with an LLM to answer questions about your code, using summaries to find relevant context at either the file or symbol level.


## API reference

| Class | Purpose |
|-------|---------|
| `DocstringIndexer` | Build / refresh the index |
| `SummarySearcher`  | Query the index |

See the [API docs](/docs/api) for full signatures.
