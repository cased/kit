"""Tools for indexing and searching LLM-generated code summaries.

This module has two public classes:

* DocstringIndexer – builds a vector index of per-file summaries generated by
  a ``kit.Summarizer``.  The index is stored in a pluggable
  :class:`kit.vector_searcher.VectorDBBackend` (default: Chroma).

* SummarySearcher  – embeds a query and retrieves the most similar summaries
  from a DocstringIndexer backend.
"""

from __future__ import annotations

import hashlib
import logging
import os
from typing import Any, Callable, Dict, List, Optional

from tqdm import tqdm

from .cache_backend import CacheBackend, CacheData
from .cache_backends.filesystem import FilesystemCacheBackend
from .repository import Repository
from .summaries import Summarizer
from .vector_searcher import ChromaDBBackend, VectorDBBackend

# Type alias for the embedding function signature
EmbedFn = Optional[Callable[[str], List[float]]]

__all__ = [
    "DocstringIndexer",
    "SummarySearcher",
]

logger = logging.getLogger(__name__)

DEFAULT_COLLECTION_NAME = "kit_docstring_index"
DEFAULT_CACHE_DIR_NAME = ".kit_cache"
DEFAULT_VECTOR_DB_DIR_NAME = "vector_db"  # Subdir for ChromaDB within the base persist dir
DEFAULT_CACHE_FILE_DIR_NAME = "docstring_cache"  # Subdir for cache metadata within the base persist dir


# --- Helper to determine default base persistence directory ---
def _get_default_base_persist_dir(repo: Repository) -> str:
    """Determines the default root directory for storing backend data."""
    if hasattr(repo, "repo_path") and repo.repo_path:
        base_dir = os.path.join(repo.repo_path, DEFAULT_CACHE_DIR_NAME)
    else:
        # Fallback if repo path isn't available (e.g., non-local repo without cache_dir?)
        base_dir = os.path.join(os.getcwd(), f"{DEFAULT_CACHE_DIR_NAME}_generic")
    os.makedirs(base_dir, exist_ok=True)
    logger.debug(f"Determined default base persist directory: {base_dir}")
    return base_dir


def _process_symbol_task(
    path_str: str,
    symbol_info: Dict[str, Any],
    summarizer_instance: Summarizer,
    embed_fn_instance: Callable[[str], List[float]],
    current_cache: Dict[str, Dict[str, str]],  # Pass for reading cache state
) -> Dict[str, Any]:
    """Processes a single symbol: summarize, embed, handle caching and errors."""
    symbol_name = symbol_info.get("name")
    symbol_type = symbol_info.get("type")

    display_name = symbol_info.get("node_path", symbol_name)
    # Ensure doc_id is formed even if symbol_name or display_name is None initially for error reporting
    doc_id_prefix = f"{path_str}::{display_name if display_name else 'unknown_symbol'}"

    if not symbol_name or not symbol_type:
        logger.debug(f"Symbol in {path_str} missing name or type, node_path: {display_name}. Skipping.")
        return {"status": "skipped_no_name_type", "doc_id": doc_id_prefix}

    doc_id = f"{path_str}::{display_name}"  # Re-assign with definite display_name
    symbol_code = symbol_info.get("code", "")

    if not symbol_code:
        logger.warning(f"Could not retrieve code for symbol {display_name} in {path_str}, skipping.")
        return {"status": "skipped_no_code", "doc_id": doc_id}

    try:
        symbol_hash = hashlib.sha1(symbol_code.encode("utf-8", "ignore")).hexdigest()
        if current_cache.get(doc_id, {}).get("hash") == symbol_hash:
            logger.debug(f"Symbol {doc_id} unchanged (hash: {symbol_hash}), skipping.")
            return {"status": "cached", "doc_id": doc_id}

        summary_text = None  # Initialize summary_text
        if symbol_type.upper() == "FUNCTION" or symbol_type.upper() == "METHOD":
            summary_text = summarizer_instance.summarize_function(path_str, display_name)
        elif symbol_type.upper() == "CLASS":
            summary_text = summarizer_instance.summarize_class(path_str, display_name)
        else:
            logger.debug(f"Symbol {doc_id} has unsupported type '{symbol_type}'. Skipping summarization.")
            return {"status": "skipped_unsupported_type", "doc_id": doc_id, "symbol_type": symbol_type}

        # Check if summarizer returned a valid string
        if not isinstance(summary_text, str):
            logger.warning(
                f"Summarizer did not return a valid string for symbol {display_name} in {path_str} (type: {symbol_type}). Received: {type(summary_text)}. Skipping."
            )
            return {"status": "skipped_invalid_summary_type", "doc_id": doc_id}

        if not summary_text.strip():
            logger.warning(f"Empty summary for symbol {display_name} in {path_str} (type: {symbol_type}), skipping.")
            return {"status": "skipped_empty_summary", "doc_id": doc_id}

        emb = embed_fn_instance(summary_text)
        meta = {
            "file_path": path_str,
            "symbol_name": display_name,
            "symbol_type": symbol_type,
            "summary": summary_text,
            "level": "symbol",
        }
        return {
            "status": "processed",
            "doc_id": doc_id,
            "embedding": emb,
            "metadata": meta,
            "hash": symbol_hash,
            "summary_for_embedding": summary_text,
        }
    except ValueError as ve:
        logger.warning(f"Skipping symbol {display_name} in {path_str} due to ValueError: {ve}")
        return {"status": "error", "doc_id": doc_id, "error_type": "ValueError", "message": str(ve)}
    except Exception as exc:
        # Log less info from worker to keep main logs cleaner, but still indicate error
        logger.error(f"Failed to process symbol {display_name} in {path_str}: {type(exc).__name__} - {exc}")
        return {"status": "error", "doc_id": doc_id, "error_type": str(type(exc).__name__), "message": str(exc)}


class DocstringIndexer:
    """Build an index of summaries (`docstrings`) for code.

    Uses a :class:`Summarizer` to generate summaries via LLM, then embeds
    these summaries using an `embed_fn`, and stores them in a vector DB
    accessed via a :class:`VectorDBBackend`. Uses a :class:`CacheBackend`
    to track file changes and avoid redundant processing.

    Args:
        repo: Repository object to index.
        summarizer: Summarizer instance for generating text summaries.
        embed_fn:
            Function `str -> List[float]` to embed summaries. Defaults to a
            sentence-transformer model.
        backend:
            Optional vector-DB backend, defaults to :class:`ChromaDBBackend`.
        cache_backend:
            Optional cache backend for storing and loading cache data.
            Defaults to :class:`FilesystemCacheBackend`.
    """

    def __init__(
        self,
        repo: Repository,
        summarizer: Summarizer,
        embed_fn: EmbedFn = None,
        *,
        backend: Optional[VectorDBBackend] = None,
        # persist_dir: Optional[str] = None, # REMOVED
        cache_backend: Optional[CacheBackend] = None,
    ) -> None:
        self.repo = repo
        self.summarizer = summarizer

        # Embedding function
        self.embed_fn: Callable[[str], List[float]]
        if embed_fn:
            self.embed_fn = embed_fn
        else:
            # Default embedding function using sentence-transformers
            try:
                from sentence_transformers import SentenceTransformer

                # Cache the model instance
                if not hasattr(DocstringIndexer, "_default_embed_model"):
                    logger.info("Initializing default SentenceTransformer model (all-MiniLM-L6-v2)")
                    DocstringIndexer._default_embed_model = SentenceTransformer("all-MiniLM-L6-v2")

                model = DocstringIndexer._default_embed_model

                def default_embed(text: str) -> List[float]:
                    embedding = model.encode(text)
                    return embedding.tolist()

                self.embed_fn = default_embed

            except ImportError:
                logger.error(
                    "SentenceTransformer embedding model requires `sentence-transformers` package. "
                    "Please install it (`pip install sentence-transformers`) or provide a custom `embed_fn`."
                )
                raise

        # Determine base directory for default backend persistence
        # We only calculate this if we actually need it for a default backend.
        self._default_base_persist_dir = None

        # Initialize the Vector DB Backend
        if backend:
            self.backend = backend
            logger.info(f"Using provided VectorDBBackend: {type(backend).__name__}")
        else:
            # Default to ChromaDBBackend
            if self._default_base_persist_dir is None:
                self._default_base_persist_dir = _get_default_base_persist_dir(self.repo)
            vector_db_persist_path = os.path.join(self._default_base_persist_dir, DEFAULT_VECTOR_DB_DIR_NAME)
            self.backend = ChromaDBBackend(persist_dir=vector_db_persist_path, collection_name=DEFAULT_COLLECTION_NAME)
            logger.info(f"Using default ChromaDBBackend. Persist path: {getattr(self.backend, 'persist_dir', 'N/A')}")

        # Initialize the Cache Backend
        if cache_backend:
            self.cache_backend = cache_backend
            logger.info(f"Using provided CacheBackend: {type(cache_backend).__name__}")
        else:
            # Default to FilesystemCacheBackend
            if self._default_base_persist_dir is None:
                self._default_base_persist_dir = _get_default_base_persist_dir(self.repo)
            cache_persist_path = os.path.join(self._default_base_persist_dir, DEFAULT_CACHE_FILE_DIR_NAME)
            self.cache_backend = FilesystemCacheBackend(persist_dir=cache_persist_path)
            logger.info(
                f"Using default FilesystemCacheBackend. Cache file dir: {getattr(self.cache_backend, 'persist_dir', 'N/A')}"
            )

    def build(self, force: bool = False, level: str = "symbol", file_extensions: Optional[List[str]] = None) -> None:
        """(Re)build the docstring index.

        Uses the configured `cache_backend` to track file changes and avoid
        re-summarizing and re-embedding unchanged code, unless `force=True`.

        Args:
            force:
                If True, ignore cache and rebuild entire index.
            level:
                Indexing level: `"symbol"` (default) or `"file"`.
            file_extensions:
                Optional list of file extensions to include (e.g., [".py", ".js"]).
                If None, all files are considered (respecting .gitignore).
        """
        # Load cache using the cache backend
        logger.debug("Loading cache using cache backend...")
        try:
            cache: CacheData = self.cache_backend.load()
            logger.info(f"Loaded {len(cache)} items from cache.")
        except Exception as e:
            logger.error(
                f"Failed to load cache using {type(self.cache_backend).__name__}: {e}. Starting with empty cache.",
                exc_info=True,
            )
            cache = {}

        if force:
            logger.info("'force=True' specified, clearing existing cache and backend entries.")
            if cache:
                try:
                    ids_to_delete = list(cache.keys())
                    if ids_to_delete:
                        logger.debug(
                            f"Attempting to delete {len(ids_to_delete)} entries from vector DB due to force rebuild."
                        )
                        self.backend.delete(ids=ids_to_delete)
                    else:
                        logger.debug(
                            "Cache was loaded but empty, no entries to delete from vector DB for force rebuild."
                        )
                except NotImplementedError:
                    logger.warning(
                        "Vector DB backend does not support delete operation. Forced rebuild might leave old entries."
                    )
                except Exception as e:
                    logger.error(f"Failed to delete entries from vector DB during force rebuild: {e}", exc_info=True)
                    # Continue with rebuild, but log the error
            # Clear the cache dictionary regardless of successful deletion
            cache = {}

        all_files = [f["path"] for f in self.repo.get_file_tree() if not f.get("is_dir", False)]

        # Filter by extension if specified
        if file_extensions:
            filtered_files = []
            for f_path in all_files:
                if any(f_path.endswith(ext) for ext in file_extensions):
                    filtered_files.append(f_path)
            files_to_process = filtered_files
            logger.info(f"Processing {len(files_to_process)} files with extensions: {file_extensions}")
        else:
            files_to_process = all_files
            logger.info(f"Processing all {len(files_to_process)} files in repository.")

        # --- Symbol Level Indexing ---
        if level == "symbol":
            logger.info("Building index at 'symbol' level.")
            embeddings: List[List[float]] = []
            metadatas: List[Dict[str, Any]] = []
            ids: List[str] = []

            for file_path in tqdm(files_to_process, desc="Indexing symbols"):
                try:
                    # Use extract_symbols which handles language detection and parsing
                    symbols = self.repo.extract_symbols(file_path)
                    if not symbols:
                        continue

                    current_content = self.repo.get_file_content(file_path)
                    current_hash = hashlib.sha256(current_content.encode()).hexdigest()

                    for symbol in symbols:
                        # Create a unique ID for the symbol embedding
                        display_name = symbol.get("node_path", symbol["name"])
                        symbol_id = f"{file_path}::{display_name}"
                        symbol_type = symbol.get("type", "UNKNOWN").upper()

                        # Check cache
                        cached_info = cache.get(symbol_id)
                        if not force and cached_info and cached_info.get("hash") == current_hash:
                            # logger.debug(f"Cache hit for symbol: {symbol_id}")
                            continue  # Skip if content hash matches

                        # Generate summary
                        summary = ""
                        if symbol_type == "FUNCTION" or symbol_type == "METHOD":
                            summary = self.summarizer.summarize_function(file_path, display_name)
                        elif symbol_type == "CLASS":
                            summary = self.summarizer.summarize_class(file_path, display_name)
                        else:
                            # Optionally handle other symbol types or skip
                            # logger.debug(f"Skipping summary for symbol type {symbol_type}: {symbol_id}")
                            continue

                        if not summary:
                            logger.warning(f"Failed to get summary for symbol: {symbol_id}")
                            continue

                        # Embed summary
                        embedding = self.embed_fn(summary)
                        if embedding is None:
                            logger.warning(f"Failed to get embedding for symbol summary: {symbol_id}")
                            continue

                        embeddings.append(embedding)
                        ids.append(symbol_id)
                        metadatas.append(
                            {
                                "file_path": file_path,
                                "symbol_name": display_name,
                                "symbol_type": symbol_type,
                                "summary": summary,
                                "level": "symbol",
                            }
                        )
                        # Update cache with new hash
                        cache[symbol_id] = {"hash": current_hash}

                except Exception as e:
                    logger.error(f"Error processing symbols in file {file_path}: {e}", exc_info=True)

        # --- File Level Indexing ---
        elif level == "file":
            logger.info("Building index at 'file' level.")
            embeddings = []
            metadatas = []
            ids = []

            for file_path in tqdm(files_to_process, desc="Indexing files"):
                try:
                    file_id = file_path  # Use file path as ID for file-level
                    current_content = self.repo.get_file_content(file_path)
                    current_hash = hashlib.sha256(current_content.encode()).hexdigest()

                    # Check cache
                    cached_info = cache.get(file_id)
                    if not force and cached_info and cached_info.get("hash") == current_hash:
                        # logger.debug(f"Cache hit for file: {file_id}")
                        continue

                    # Generate summary for the whole file
                    summary = self.summarizer.summarize_file(file_path)
                    if not summary:
                        logger.warning(f"Failed to get summary for file: {file_path}")
                        continue

                    # Embed summary
                    embedding = self.embed_fn(summary)
                    if embedding is None:
                        logger.warning(f"Failed to get embedding for file summary: {file_path}")
                        continue

                    embeddings.append(embedding)
                    ids.append(file_id)
                    metadatas.append(
                        {
                            "file_path": file_path,
                            "summary": summary,
                            "level": "file",
                        }
                    )
                    # Update cache
                    cache[file_id] = {"hash": current_hash}

                except Exception as e:
                    logger.error(f"Error processing file {file_path}: {e}", exc_info=True)

        else:
            raise ValueError(f"Invalid indexing level specified: '{level}'. Must be 'symbol' or 'file'.")

        # Upsert embeddings to vector DB if any were generated
        if embeddings and ids and metadatas:
            logger.info(f"Upserting {len(ids)} new/updated embeddings to vector DB...")
            try:
                self.backend.upsert(embeddings, metadatas, ids)
            except Exception as e:
                logger.error(f"Failed to upsert embeddings to vector DB: {e}", exc_info=True)
        else:
            logger.warning("No embeddings were generated during indexing.")

        # Persist cache using the cache backend
        logger.debug("Saving cache using cache backend...")
        try:
            self.cache_backend.save(cache)
            logger.info(f"Saved {len(cache)} items to cache.")
        except Exception as e:
            logger.error(f"Failed to save cache using {type(self.cache_backend).__name__}: {e}", exc_info=True)

    def get_searcher(self) -> "SummarySearcher":
        """
        Get a :class:`SummarySearcher` instance configured for this index.

        Returns:
            A SummarySearcher instance.
        """
        return SummarySearcher(self)


class SummarySearcher:
    """Search an index built by :class:`DocstringIndexer`."""

    def __init__(self, indexer: "DocstringIndexer"):
        self.indexer = indexer

    def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """Embed the query and search the vector DB.

        Args:
            query:
                The search query.
            top_k:
                Number of results to return.

        Returns:
            List of search results, each with keys like `id`, `score`, `summary`,
            `file_path`, etc.
        """
        embedding = self.indexer.embed_fn(query)  # type: ignore
        if embedding is None:
            return []
        results = self.indexer.backend.search(embedding, top_k=top_k)
        logger.debug(f"Retrieved {len(results)} results for query '{query[:50]}...'")
        # Hydrate results with summaries stored in metadata
        for res in results:
            if "summary" not in res and "metadata" in res and res["metadata"]:
                res["summary"] = res["metadata"].get("summary", "")
        return results
