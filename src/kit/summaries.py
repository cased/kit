"""Handles code summarization using LLMs."""

import os
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Optional, Any

# Use TYPE_CHECKING to avoid circular import issues with Repository
if TYPE_CHECKING:
    from kit.repository import Repository
    from kit.repo_mapper import RepoMapper # For type hinting


class LLMError(Exception):
    """Custom exception for LLM related errors."""
    pass


class SymbolNotFoundError(Exception):
    """Custom exception for when a symbol (function, class) is not found."""
    pass


@dataclass
class OpenAIConfig:
    """Configuration for OpenAI API access."""
    api_key: Optional[str] = field(default_factory=lambda: os.environ.get("OPENAI_API_KEY"))
    model: str = "gpt-4o"
    temperature: float = 0.7
    max_tokens: int = 1000  # Default max tokens for summary

    def __post_init__(self):
        if not self.api_key:
            raise ValueError(
                "OpenAI API key not found. "
                "Set OPENAI_API_KEY environment variable or pass api_key directly."
            )


class Summarizer:
    """Provides methods to summarize code using a configured LLM."""

    def __init__(self, repo: 'Repository', config: Optional[OpenAIConfig] = None, llm_client: Optional[Any] = None):
        """
        Initializes the Summarizer.

        Args:
            repo: The kit.Repository instance containing the code.
            config: LLM configuration (currently OpenAIConfig). Defaults to
                    OpenAIConfig loading from environment variables. The config
                    can specify 'model', 'temperature', and 'max_tokens'.
            llm_client: Optional pre-configured/mock client for testing.
        """
        self.repo = repo
        self.config = config or OpenAIConfig()  # Default to OpenAI env var config
        # Allow injecting a pre-configured/mock client (useful for tests)
        self._llm_client = llm_client

        if not isinstance(self.config, OpenAIConfig):
            # Extend this later for other config types
            raise NotImplementedError("Only OpenAIConfig is currently supported.")

    def _get_llm_client(self):
        """Lazy loads the OpenAI client."""
        if self._llm_client is not None:
            return self._llm_client

        try:
            from openai import OpenAI  # Local import to avoid hard dependency
        except ImportError as e:
            raise ImportError(
                "OpenAI client not found. Install with 'pip install kit[openai]' or 'pip install openai'"
            ) from e

        self._llm_client = OpenAI(api_key=self.config.api_key)
        return self._llm_client

    def summarize_file(self, file_path: str) -> str:
        """
        Summarizes the content of the specified file using the configured LLM.

        Args:
            file_path: The path to the file within the repository.

        Returns:
            The summary text generated by the LLM.

        Raises:
            FileNotFoundError: If the file_path does not exist in the repo.
            LLMError: If there's an issue during LLM interaction.
        """
        # 1. Get file content from repo
        try:
            # Assuming repo has a method like this - we might need to add it
            content = self.repo.get_file_content(file_path) 
        except FileNotFoundError:
             raise # Re-raise the FileNotFoundError
        except Exception as e:
             # Handle other potential repo errors if necessary
             raise RuntimeError(f"Error reading file {file_path} from repo: {e}") from e

        # 2. Construct the prompt (basic example)
        prompt = f"""Please provide a concise summary of the following code file: `{file_path}`.
Focus on its primary purpose, key functionalities, and how it fits into a larger project if evident.

Code:
```
{content}
```

Summary:"""

        # 3. Get LLM client (lazy loaded)
        client = self._get_llm_client()

        # 4. Call LLM API
        try:
            response = client.chat.completions.create(
                model=self.config.model,
                messages=[
                    {"role": "system", "content": "You are an expert assistant skilled in creating concise and informative code summaries."},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
            )
            summary = response.choices[0].message.content
            if not summary:
                 raise LLMError("LLM returned an empty summary.")
            return summary.strip()
        
        except Exception as e:
            # Catch potential API errors, connection issues, etc.
            raise LLMError(f"Error communicating with OpenAI API: {e}") from e

    def summarize_function(self, file_path: str, function_name: str) -> str:
        """
        Summarizes a specific function within the specified file.

        Args:
            file_path: The path to the file containing the function.
            function_name: The name of the function to summarize.

        Returns:
            The summary text generated by the LLM.

        Raises:
            FileNotFoundError: If the file_path does not exist in the repo.
            SymbolNotFoundError: If the function is not found in the file.
            LLMError: If there's an issue during LLM interaction.
        """
        try:
            file_content = self.repo.get_file_content(file_path)
            repo_mapper: 'RepoMapper' = self.repo.get_repo_mapper() # Assumes Repository provides this
            symbols = repo_mapper.extract_symbols(file_path) # Gets symbols for this specific file
        except FileNotFoundError:
            raise
        except Exception as e:
            raise RuntimeError(f"Error preparing to summarize function {function_name} in {file_path}: {e}") from e

        target_symbol_code = None
        for symbol in symbols:
            # Assuming symbol dict has 'name', 'kind' (e.g., 'function', 'method'), 'start_byte', 'end_byte'
            # Adjust 'kind' based on actual output of TreeSitterSymbolExtractor
            if symbol.get('name') == function_name and symbol.get('kind') in ['function', 'method']:
                start = symbol['start_byte']
                end = symbol['end_byte']
                target_symbol_code = file_content[start:end]
                break
        
        if target_symbol_code is None:
            raise SymbolNotFoundError(f"Function '{function_name}' not found in '{file_path}'.")

        prompt = f"""Please provide a concise summary of the following function named `{function_name}` from the file `{file_path}`.
Focus on its purpose, parameters, return value, and key logic.

Function Code:
```
{target_symbol_code}
```

Summary:"""

        client = self._get_llm_client()
        try:
            response = client.chat.completions.create(
                model=self.config.model,
                messages=[
                    {"role": "system", "content": "You are an expert assistant skilled in creating concise code summaries for functions."},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
            )
            summary = response.choices[0].message.content
            if not summary:
                 raise LLMError(f"LLM returned an empty summary for function {function_name}.")
            return summary.strip()
        except Exception as e:
            raise LLMError(f"Error communicating with OpenAI API for function {function_name}: {e}") from e

    def summarize_class(self, file_path: str, class_name: str) -> str:
        """
        Summarizes a specific class within the specified file.

        Args:
            file_path: The path to the file containing the class.
            class_name: The name of the class to summarize.

        Returns:
            The summary text generated by the LLM.

        Raises:
            FileNotFoundError: If the file_path does not exist in the repo.
            SymbolNotFoundError: If the class is not found in the file.
            LLMError: If there's an issue during LLM interaction.
        """
        try:
            file_content = self.repo.get_file_content(file_path)
            repo_mapper: 'RepoMapper' = self.repo.get_repo_mapper() # Assumes Repository provides this
            symbols = repo_mapper.extract_symbols(file_path)
        except FileNotFoundError:
            raise
        except Exception as e:
            raise RuntimeError(f"Error preparing to summarize class {class_name} in {file_path}: {e}") from e

        target_symbol_code = None
        for symbol in symbols:
            if symbol.get('name') == class_name and symbol.get('kind') == 'class': # Adjust 'kind' as needed
                start = symbol['start_byte']
                end = symbol['end_byte']
                target_symbol_code = file_content[start:end]
                break
        
        if target_symbol_code is None:
            raise SymbolNotFoundError(f"Class '{class_name}' not found in '{file_path}'.")

        prompt = f"""Please provide a concise summary of the following class named `{class_name}` from the file `{file_path}`.
Focus on its primary responsibilities, key attributes, and important methods.

Class Code:
```
{target_symbol_code}
```

Summary:"""

        client = self._get_llm_client()
        try:
            response = client.chat.completions.create(
                model=self.config.model,
                messages=[
                    {"role": "system", "content": "You are an expert assistant skilled in creating concise code summaries for classes."},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
            )
            summary = response.choices[0].message.content
            if not summary:
                 raise LLMError(f"LLM returned an empty summary for class {class_name}.")
            return summary.strip()
        except Exception as e:
            raise LLMError(f"Error communicating with OpenAI API for class {class_name}: {e}") from e
