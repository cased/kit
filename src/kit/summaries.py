"""Handles code summarization using LLMs."""

import os
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Optional, Any

# Use TYPE_CHECKING to avoid circular import issues with Repository
if TYPE_CHECKING:
    from kit.repository import Repository


class LLMError(Exception):
    """Custom exception for LLM related errors."""
    pass


@dataclass
class OpenAIConfig:
    """Configuration for OpenAI API access."""
    api_key: Optional[str] = field(default_factory=lambda: os.environ.get("OPENAI_API_KEY"))
    model: str = "gpt-4o" # Or a cheaper/faster default like gpt-3.5-turbo?
    # Add other potential OpenAI params like temperature, max_tokens later

    def __post_init__(self):
        if not self.api_key:
            raise ValueError(
                "OpenAI API key not found. "
                "Set OPENAI_API_KEY environment variable or pass api_key directly."
            )

# Placeholder for other configs later
# @dataclass
# class AnthropicConfig: ...

# --- Summarizer Class --- #

class Summarizer:
    """Provides methods to summarize code using a configured LLM."""

    def __init__(self, repo: 'Repository', config: Optional[OpenAIConfig] = None, llm_client: Optional[Any] = None):
        """
        Initializes the Summarizer.

        Args:
            repo: The kit.Repository instance containing the code.
            config: LLM configuration (currently OpenAIConfig). Defaults to 
                    OpenAIConfig loading from environment variables.
            llm_client: Optional pre-configured/mock client for testing.
        """
        self.repo = repo
        self.config = config or OpenAIConfig()  # Default to OpenAI env var config
        # Allow injecting a pre-configured/mock client (useful for tests)
        self._llm_client = llm_client

        if not isinstance(self.config, OpenAIConfig):
            # Extend this later for other config types
            raise NotImplementedError("Only OpenAIConfig is currently supported.")

    def _get_llm_client(self):
        """Lazy loads the OpenAI client."""
        if self._llm_client is not None:
            return self._llm_client

        try:
            from openai import OpenAI  # Local import to avoid hard dependency
        except ImportError as e:
            raise ImportError(
                "OpenAI client not found. Install with 'pip install kit[openai]' or 'pip install openai'"
            ) from e

        self._llm_client = OpenAI(api_key=self.config.api_key)
        return self._llm_client

    def summarize_file(self, file_path: str) -> str:
        """
        Summarizes the content of the specified file using the configured LLM.

        Args:
            file_path: The path to the file within the repository.

        Returns:
            The summary text generated by the LLM.

        Raises:
            FileNotFoundError: If the file_path does not exist in the repo.
            LLMError: If there's an issue during LLM interaction.
        """
        # 1. Get file content from repo
        try:
            # Assuming repo has a method like this - we might need to add it
            content = self.repo.get_file_content(file_path) 
        except FileNotFoundError:
             raise # Re-raise the FileNotFoundError
        except Exception as e:
             # Handle other potential repo errors if necessary
             raise RuntimeError(f"Error reading file {file_path} from repo: {e}") from e

        # 2. Construct the prompt (basic example)
        prompt = f"Summarize the following code file ({file_path}):\n\n```\n{content}\n```\n\nProvide a concise summary of its purpose and key functionality."

        # 3. Get LLM client (lazy loaded)
        client = self._get_llm_client()

        # 4. Call LLM API
        try:
            response = client.chat.completions.create(
                model=self.config.model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant skilled at summarizing code."},
                    {"role": "user", "content": prompt}
                ]
                # Add other params like temperature, max_tokens from config later
            )
            summary = response.choices[0].message.content
            if not summary:
                 raise LLMError("LLM returned an empty summary.")
            return summary.strip()
        
        except Exception as e:
            # Catch potential API errors, connection issues, etc.
            raise LLMError(f"Error communicating with OpenAI API: {e}") from e

    # --- Placeholder for future methods ---
    # def summarize_function(self, file_path: str, function_name: str) -> str:
    #     raise NotImplementedError

    # def summarize_class(self, file_path: str, class_name: str) -> str:
    #     raise NotImplementedError
